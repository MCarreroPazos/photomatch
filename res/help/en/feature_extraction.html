<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<title>Feature extraction</title>
<meta http-equiv="Content-Script-Type" content="text/javascript">
<meta http-equiv="Content-Style-Type" content="text/css">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="stylesheet" type="text/css" href="css/styles.css">
</head>
<body>

  <h1 class="western">Feature extraction</h1>
   
  <div class="image"><img src="img/feature_extraction.png" width="352" height="482" /> 
    <br clear="all" />
  </div>
  
  <h3 class="western">Detector and descriptors</h3>
  
  <ul>  
    <li class="western"><a href="#AGAST">AGAST</a></li>
    <li class="western"><a href="#AKAZE">AKAZE</a></li>
    <li class="western"><a href="#BOOST">BOOST</a></li>
    <li class="western"><a href="#BRIEF">BRIEF</a></li>
    <li class="western"><a href="#BRISK">BRISK</a></li>
    <li class="western"><a href="#DAISY">DAISY</a></li>
    <li class="western"><a href="#FAST">FAST</a></li>
    <li class="western"><a href="#FREAK">FREAK</a></li>
    <li class="western"><a href="#GFTT">GFTT</a></li>
    <li class="western"><a href="#HOG">HOG</a></li>
    <li class="western"><a href="#KAZE">KAZE</a></li>
    <li class="western"><a href="#LATCH">LATCH</a></li>
    <li class="western"><a href="#LSS">LSS</a></li>
    <li class="western"><a href="#MSD">MSD</a></li>
    <li class="western"><a href="#MSER">MSER</a></li>
    <li class="western"><a href="#ORB">ORB</a></li>
    <li class="western"><a href="#SIFT">SIFT</a></li>
    <li class="western"><a href="#STAR">STAR</a></li>
    <li class="western"><a href="#SURF">SURF</a></li>
    <li class="western"><a href="#VGG">VGG</a></li>
  </ul>      
    
    <!-- AGAST -->
    
    <h3 class="western"><a name="AGAST">AGAST (Detector)</a></h3>
    
    <p class="western"><strong>Adaptive and Generic Corner Detection Based on the Accelerated Segment Test</strong></p>
    
    <p class="western" align="justify">Corner detector. It is an optimization of FAST, thus also based on Accelerated Segment Test (AST), but its decision tree is generic, with no need to retrain the it every time.</p>
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/agast.png" width="344" height="110" /> 
      <br clear="all" />
    </div>
  
    <ul>
      <li class="western"><strong>Threshold</strong>: The AST applies a minimum difference threshold when comparing the value of a pixel on the pattern with the brightness of the nucleus. This parameter controls the sensitivity of the corner response.</li>
      <li class="western"><strong>Nonmax Suppression</strong>: Non Maximal Suppression for removing adjacent corners.</li>
      <li class="western"><strong>Detector type</strong>: Suported types are:
      <ul>
        <li class="western"><strong>AGAST_5_8</strong>: AGAST-5 decision tree with the 8 pixels mask</li>
        <li class="western"><strong>AGAST_7_12d</strong>: AGAST-7 decision tree with the Diamond shaped 12 pixels mask</li>
        <li class="western"><strong>AGAST_7_12s</strong>: AGAST-7 decision tree with the Squared shaped 12 pixels mask</li>
        <li class="western"><strong>OAST_9_16</strong>: OAST-9 (Optimal AST) decision tree with the 16 pixels mask</li>
      </ul>
      </li>
    </ul>
  
    <p class="western" align="justify">[<a href="https://mediatum.ub.tum.de/doc/1287456/file.pdf">AGAST</a>]: <em>Mair, E., Hager, G.D., Burschka, D., Suppa, M. and Hirzinger, G., 2010. Adaptive and generic corner detection based on the accelerated segment test. In European conference on Computer vision, pp. 183-196. Springer, Berlin, Heidelberg.</em></p>
      
      
    <!-- AKAZE -->
    
    <h3 class="western"><a name="AKAZE">AKAZE (Detector/Descriptor)</a></h3>
    
    <p class="western"><strong>Accelerated KAZE</strong></p>
    
    <p class="western" align="justify">In a similar fashion with KAZE, it operates in nonlinear scale space and not in Gaussian like SIFT and SURF do. Numerical methods are used to approximate the solution, namely Fast Explicit Diffusion (FED), that are proven to work much faster than any other discretization scheme.</p>

    <p class="western" align="justify">A highly efficient Modified-Local Difference Binary (M-LDB) descriptor that exploits gradient and intensity information from the nonlinear scale space. The LDB descriptor follows the same principle as BRIEF, but using binary tests between the average of areas instead of single pixels for additional robustness. M-LDB uses the derivatives computed in the feature detection step, reducing the number of operations required to construct the descriptor</p>
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/akaze.png" width="341" height="213" /> 
      <br clear="all" />
    </div>
    
    <ul>
      <li class="western"><strong>Descriptor Type</strong>: Type of the extracted descriptor (<em>KAZE</em>,&nbsp;<em>KAZE_UPRIGHT</em>, <em>MLDB</em> or <em>MLDB_UPRIGHT</em>).</li>
      <li class="western"><strong>Descriptor Size</strong>:&nbsp;Size of the descriptor in bits.</li>
      <li class="western"><strong>Detector Channels</strong>: Number of channels in the descriptor (1, 2 or 3 (default)).&nbsp;</li>
      <li class="western"><strong>Threshold</strong>: Detector response threshold to accept point.</li>
      <li class="western"><strong>Octaves</strong>:&nbsp;Number of octaves.</li>
      <li class="western"><strong>Octave Layers</strong>:&nbsp;Default number of sublevels per scale level.</li>
      <li class="western"><strong>Diffusivity</strong>:&nbsp;Diffusivity type (<em>DIFF_PM_G1</em>, <em>DIFF_PM_G2</em> (default), <em>DIFF_WEICKERT</em> or <em>DIFF_CHARBONNIER</em>)</li>
    </ul>

    <p class="western" align="justify">[<a href="http://www.bmva.org/bmvc/2013/Papers/paper0013/abstract0013.pdf">AKAZE</a>]: <em>Alcantarilla, P.F., Nuevo, J., Bartoli, A., 2013. Fast explicit diffusion for accelerated features in nonlinear scale spaces. In Proc. BMVC, Vol. 34(7), pp. 1281â€“1298.</em></p>
    
    
    
    <!-- BOOST -->
    
    <h3 class="western"><a name="BOOST">BOOST (Descriptor)</a></h3>
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/boost.png" width="340" height="83" /> 
      <br clear="all" />
    </div>    
    
    <ul>
      <li class="western"><strong>Descriptor Type</strong>:&nbsp;Type of descriptor used (BINBOOST_256, BGM, BGM_HARD, BGM_BILINEAR, LBGM, BINBOOST_64, BINBOOST_128, BINBOOST_256)</li>
      <li class="western"><strong>Use Keypoints Orientation</strong>: Sample patterns using keypoints orientation.</li>
    </ul>
    
    
    
    <!-- BRIEF -->
    
    
    <h3 class="western"><a name="BRIEF">BRIEF (Descriptor)</a></h3>
    
    <p class="western" align="justify"><strong>Binary Robust Independent Elementary Features</strong></p>
    
    <p class="western" align="justify">A short binary descriptor using the hamming distance. Not invariant to scale and rotation.</p>
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/brief.png" width="341" height="85" /> 
      <br clear="all" />
    </div>
    
    <ul>
      <li class="western"><strong>Descriptor Bytes</strong>: Legth of the descriptor in bytes.&nbsp;Valid values are: 16, 32 (default) or 64</li>
      <li class="western"><strong>Use Keypoints Orientation</strong>: Sample patterns using keypoints orientation.</li>
    </ul>
    
    
    <p class="western" align="justify">[<a href="https://ieeexplore.ieee.org/abstract/document/6081878">BRIEF</a>]: <em>Calonder, M., Lepetit, V., Ozuysal, M., Trzcinski, T., Strecha, C. and Fua, P., 2011. BRIEF: Computing a local binary descriptor very fast. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 34(7), pp.1281-1298.</em></p>
    
    
    
    <!-- BRISK -->
    
    <h3 class="western"><a name="BRISK">BRISK (Detector/Descriptor)</a></h3>

    <p class="western" align="justify"><strong>Binary Robust Invariant Scalable Keypoints</strong></p>
    
    <p class="western" align="justify">A sampling pattern consisting of points lying on appropriately scaled concentric circles is applied at the neighborhood of each keypoint to retrieve gray values: processing local intensity gradients,the feature characteristic direction is determined. The oriented BRISK sampling pattern is used to obtain pairwise brightness comparison results which are assembled into the binary BRISK descriptor.</p>
    
    <p class="western" align="justify"></p> 
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/brisk.png" width="341" height="113" /> 
      <br clear="all" />
    </div>
    
    <ul>
      <li class="western"><strong>Threshold</strong>: AGAST detection threshold score (Default=30).</li>
      <li class="western"><strong>Octaves:</strong> Detection octaves (Default=3).</li>
      <li class="western"><strong>Pattern Scale</strong>:&nbsp;</li>
    </ul>

    <p class="western" align="justify">[<a href="https://ieeexplore.ieee.org/abstract/document/6081878">BRISK</a>]: <em>Leutenegger, S., Chli, M. and Siegwart, R., 2011. BRISK: Binary robust invariant scalable keypoints. In 2011 IEEE International Conference on Computer Vision (ICCV), pp. 2548-2555. IEEE.</em></p>  
    
    
    <!-- DAISY -->
    
    <h3 class="western"><a name="DAISY">DAISY  (Descriptor)</a></h3>
    
    <p class="western" align="justify">Feature descriptor that depends on histograms of gradients like SIFT but uses a Gaussian weighting and circularly symmetrical kernel.</p>
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/daisy.png" width="354" height="213" /> 
      <br clear="all" />
    </div>
    
    <ul>
      <li class="western"><strong>Radius</strong>: Radius of the descriptor at the initial scale (Default=15.).</li>
      <li class="western"><strong>Radial range division quantity</strong>.</li>
      <li class="western"><strong>Angular range division quantity</strong>.</li>
      <li class="western"><strong>Interpolation</strong></li>
      <li><strong>Keypoints orientation</strong></li>
      <li><strong>Descriptor normalization type</strong>:&nbsp;Normalization types are:
      <ul>
        <li>NRM_NONE: not make any normalization (Default).</li>
        <li>NRM_PARTIAL: mean that histograms are normalized independently for L2 norm equal to 1.0.</li>
        <li>NRM_FULL: mean that descriptors are normalized for L2 norm equal to 1.0.</li>
        <li>NRM_SIFT: mean that descriptors are normalized for L2 norm equal to 1.0 but no individual one is bigger than 0.154 as in SIFT.</li>
      </ul>
      </li>
    </ul>

    <p class="western" align="justify">[<a href="https://ieeexplore.ieee.org/abstract/document/4815264">DAISY</a>]: <em>Tola, E., Lepetit, V. and Fua, P., 2009. Daisy: An efficient dense descriptor applied to wide-baseline stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 32(5), pp. 815-830.</em></p>
    
    
    <!-- FAST -->
    
    <h3 class="western"><a name="FAST">FAST  (Detector)</a></h3>

    <p class="western" align="justify"><strong>Features from Accelerated Segment Test</strong></p>
    
    <p class="western" align="justify">Modification of the SUSAN corner detector that outperforms previously used detectors in terms of speed and reliability. Is based on Accelerated Segment Test (AST), which is used to distinguish keypoints by examining the intensity values of 16 pixels that fall in the circular pattern around the candidate pixel. A candidate pixel is considered as keypoint if there are at least N continuous pixels that have either higher or lower intensity values than it.</p> 
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/fast.png" width="351" height="111" /> 
      <br clear="all" />
    </div>
    
    <ul>
      <li class="western"><strong>Threshold</strong></li>
      <li class="western"><strong>Nonmax Suppression</strong>: Non Maximal Suppression for removing adjacent corners.</li>
      <li class="western"><strong>Detector type</strong>: Suported types are:
        <ul>
          <li class="western"><strong>TYPE_5_8</strong>: FAST-5 decision tree whith the 8 pixels mask</li>
          <li class="western"><strong>TYPE_7_12</strong>: FAST-7 decision tree whith the 12 pixels mask</li>
          <li class="western"><strong>TYPE_9_16</strong>: FAST-9 decision tree whith the 16 pixels mask (default)</li>
        </ul>
      </li>
    </ul>

    <p class="western" align="justify">[<a href="https://link.springer.com/chapter/10.1007/11744023_34">FAST</a>]: <em>Rosten, E. and Drummond, T., 2006. Machine learning for high-speed corner detection. In European conference on computer vision, pp. 430-443. Springer, Berlin, Heidelberg.</em></p>
    
     
     
    <!-- FREAK -->
     
    <h3 class="western"><a name="FREAK">FREAK (Descriptor)</a></h3>
    
    <p class="western" align="justify"><strong>Fast Retina Keypoint</strong></p>
    
    <p class="western" align="justify">The algorithm propose a novel keypoint descriptor inspired by the human visual system and more precisely the retina. A cascade of binary strings is computed by efficiently comparing image intensities over a retinal sampling pattern. FREAK keypoints are in general faster to compute with lower memory load and also more robust than SIFT, SURF or BRISK. They are competitive alternatives to existing keypoints in particular for embedded applications.</p> 
   
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/freak.png" width="353" height="134" /> 
      <br clear="all" />
    </div>
    
    <ul>
      <li class="western"><strong>Orientation Normalization</strong></li>
      <li class="western"><strong>Scale Normalization:</strong></li>
      <li class="western"><strong>Pattern Scale</strong>:&nbsp;</li>
      <li class="western"><strong>Number Of Octaves</strong>:&nbsp;</li>
    </ul>
       
    <p class="western" align="justify">[<a href="https://ieeexplore.ieee.org/abstract/document/6247715">FREAK</a>]: <em>Alahi, A., Ortiz, R. and Vandergheynst, P., 2012. Freak: Fast retina keypoint. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 510-517. IEEE.</em></p>
        
        
      
    <!-- GFTT -->       
        
    <h3 class="western"><a name="GFTT">GFTT (Detector)</a></h3>
    
    <p class="western" align="justify"><strong>Good Features to Track</strong></p>
    
    <p class="western" align="justify">Modified version of the traditional Harris detector as to select only the features that pass the dissimilarity (change of appearance) test and filter out the less robust ones.</p> 
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/gftt.png" width="350" height="188" /> 
      <br clear="all" />
    </div>
    
    <ul>
      <li class="western"><strong>Max Features</strong></li>
      <li class="western"><strong>Quality Level:</strong></li>
      <li class="western"><strong>Min Distance</strong>:&nbsp;</li>
      <li class="western"><strong>Block Size</strong>:</li>
      <li class="western"><strong>Harris Detector</strong></li>
      <li class="western"><strong>K</strong></li>
    </ul>

    <p class="western" align="justify">[<a href="http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf">GFTT</a>]: <em>Harris, C.G. and Stephens, M., 1988. A combined corner and edge detector. In Alvey vision conference, Vol. 15(50), pp. 10-5244.</em></p>
    
    
    
    <!-- HOG -->
    
    <h3 class="western"><a name="HOG">HOG (Descriptor)</a></h3>
    
    <p class="western" align="justify"><strong>Histogram of Oriented Gradients</strong></p>
    
    <p class="western" align="justify">Based on image gradients, histograms of gradients are calculated for pre-defiend image sub-regions. Mainly used for pedestrian detection, sensitive to rotation changes.</p> 
        
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/hog.png" width="352" height="191" /> 
      <br clear="all" />
    </div>
    
    <ul>
      <li class="western"><strong>Window size</strong>: Width and height (default = [16,16]).</li>
      <li class="western"><strong>Block Size</strong>: (default = [4,4])</li>
      <li class="western"><strong>Block Stride</strong>: (default = [2,2])</li>
      <li class="western"><strong>Cell Size</strong>: (default = [2,2])</li>
      <li class="western"><strong>nbins</strong>: (default = 9)</li>
      <li class="western"><strong>derivAperture</strong>: (default = 1)</li>
    </ul>

    
    <!-- KAZE -->
            
    <h3 class="western"><a name="KAZE">KAZE (Detector/Descriptor)</a></h3>
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/kaze.png" width="354" height="188" /> 
      <br clear="all" />
    </div>
    
    <p class="western" align="justify">[<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.304.4980&rep=rep1&type=pdf">KAZE</a>]: <em>Alcantarilla, P.F., Bartoli, A. and Davison, A.J., 2012. KAZE features. In European Conference on Computer Vision, pp. 214-227. Springer, Berlin, Heidelberg.</em></p>
    
    
    <!-- LATCH -->
    
    <h3 class="western"><a name="LATCH">LATCH (Descriptor)</a></h3>
    
    <p class="western" align="justify"><strong>Learned Arrangements of Three Patch Codes</strong></p>
    
    <p class="western" align="justify">LATCH is a binary descriptor based on learned comparisons of triplets of image patches.</p> 
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/latch.png" width="352" height="110" /> 
      <br clear="all" />
    </div>
    
    <p class="western" align="justify">[<a href="https://arxiv.org/pdf/1501.03719.pdf">LATCH</a>]: <em>Levi, G. and Hassner, T., 2016. LATCH: learned arrangements of three patch codes. In 2016 IEEE winter conference on applications of computer vision (WACV), pp. 1-9. IEEE.</em></p>
    
    
    
    <!-- LSS -->    
    
    <h3 class="western"><a name="LSS">LSS (Descriptor)</a></h3>

    <p class="western" align="justify"><strong>Local Self-Similarity</strong></p>
    
    <p class="western" align="justify">Local Self-Similarity is based on the texture features of images to densely calculate local self-similarity descriptors.</p> 
    
    <p class="western" align="justify">[<a href="http://image.ntua.gr/iva/files/ShechtmanIrani_CVPR2007%20-%20Matching%20Local%20Self-Similarities%20Across%20Images%20and%20Videos.pdf">LSS</a>]: <em>Shechtman, E. and Irani, M., 2007. Matching Local Self-Similarities across Images and Videos. In CVPR, Vol. 2, pp. 3.</em></p>
    
    
    
    <!-- MSD --> 
    
    <h3 class="western"><a name="MSD">MSD (Detector)</a></h3>
    
    <p class="western" align="justify"><strong>Maximal Self-Dissimilarity</strong></p>
    
    <p class="western" align="justify">The algorithm implements a novel interest point detector stemming from the intuition that image patches which are highly dissimilar over a relatively large extent of their surroundings hold the property of being repeatable and distinctive. This concept of "contextual self-dissimilarity" reverses the key paradigm of recent successful techniques such as the Local Self-Similarity descriptor and the Non-Local Means filter, which build upon the presence of similar - rather than dissimilar - patches. Moreover, it extends to contextual information the local self-dissimilarity notion embedded in established detectors of corner-like interest points, thereby achieving enhanced repeatability, distinctiveness and localization accuracy.</p> 
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/msd.png" width="352" height="192" /> 
      <br clear="all" />
    </div>
    
    <p class="western" align="justify">[<a href="http://imagine.enpc.fr/~monasse/Stereo/Projects/TombariDiStefano14.pdf">MSD</a>]: <em>Tombari, F. and Di Stefano, L., 2014. Interest points via maximal self-dissimilarities. In Asian Conference on Computer Vision, pp. 586-600. Springer, Cham.</em></p>
    
    
    <!-- MSER -->    
    
    <h3 class="western"><a name="MSER">MSER (Detector)</a></h3>
    
    <p class="western" align="justify"><strong>Maximally Stable Extremal Region Extractor.</strong></p>
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/mser.png" width="352" height="269" /> 
      <br clear="all" />
    </div>
    
    
    
    <!-- ORB -->    
    
    <h3 class="western"><a name="ORB">ORB (Detector/Descriptor)</a></h3>
    
    <p class="western" align="justify"><strong>Oriented FAST and Rotated BRIEF</strong></p>
    
    <p class="western" align="justify">The algorithm uses FAST in pyramids to detect stable keypoints, selects the strongest features using FAST or Harris response, finds their orientation using first-order moments and computes the descriptors using BRIEF (where the coordinates of random point pairs (or k-tuples) are rotated according to the measured orientation).</p> 
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/orb.png" width="352" height="242" /> 
      <br clear="all" />
    </div>
    
    <p class="western" align="justify">[<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.370.4395&rep=rep1&type=pdf">ORB</a>]: <em>Rublee, E., Rabaud, V., Konolige, K. and Bradski, G.R., 2011. ORB: An efficient alternative to SIFT or SURF. In ICCV, Vol. 11(1), pp. 2.</em></p>
    
    
    
    <!-- SIFT --> 
    
    <h3 class="western"><a name="SIFT">SIFT (Detector/Descriptor)</a></h3>
    
    <p class="western" align="justify"><strong>Scale Invariant Feature Transform</strong></p>
    
    <p class="western" align="justify">Detector part of SIFT algorithm uses Difference of Gaussians (DoG), a feature enhancement method, where image pyramids are created by repeatedly convolving the original image with Gaussian kernels. In each pyramid level, every pixel is compared with its 8 neighboring pixels in the current image as well as with its 9 neighbors of its adjacent pyramid images. Keypoints are detected as extrema in the difference between the Gaussian images.</p> 
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/sift.png" width="351" height="166" /> 
      <br clear="all" />
    </div>
    
    <ul>
      <li class="western"><strong>Features Number</strong>: The number of best features to retain. The features are ranked by their scores (measured in&nbsp;SIFT algorithm as the local contrast).</li>
      <li class="western"><strong>Octave Layers</strong>: Number of layers in each octave.&nbsp;3 is the value used in D. Lowe paper. The number of octaves&nbsp;is computed automatically from the image resolution.</li>
      <li class="western"><strong>Contrast Threshold</strong>: Used to filter out weak features in semi-uniform (low-contrast) regions.</li>
      <li class="western"><strong>Edge&nbsp;Threshold</strong>:&nbsp;Threshold used to filter out edge-like features</li>
      <li class="western"><strong>Sigma</strong>: Sigma of the Gaussian applied to the input image at the octave 0.</li>
    </ul>

  <p class="western" align="justify">[<a href="https://link.springer.com/article/10.1023/B:VISI.0000029664.99615.94">SIFT</a>]: <em>Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoints. International journal of computer vision, Vol. 60(2), pp.91-110.</em></p>

    <!-- STAR --> 
    
    <h3 class="western"><a name="STAR">STAR (Detector)</a></h3>
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/star.png" width="353" height="165" /> 
      <br clear="all" />
    </div>
    
    
    <!-- SURF --> 
    
    
    <h3 class="western"><a name="SURF">SURF (Detector/Descriptor)</a></h3>
    
    <p class="western" align="justify"><strong>Speeded-Up Robust Features</strong></p>
    
    <p class="western" align="justify">SURF uses Fast Hessian as a detection method that is based on integral images through Hessian matrix approximation. Box type convolution filters of different sizes are used to approximate second order Gaussian derivatives for each image point. Keypoints are regions where the determinant becomes maximal through non-maximal suppression.</p>
    
    <p class="western" align="justify">The description part of SURF describes the intensity of the neighborhood around the pixel using Haar wavelets.</p>

    <p class="western" align="justify">[<a href="https://link.springer.com/chapter/10.1007/11744023_32">SURF</a>]: Bay, H., Tuytelaars, T. and Van Gool, L., 2006. Surf: Speeded up robust features. In European conference on computer vision, pp. 404-417. Springer, Berlin, Heidelberg.</p>
    
    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/surf.png" width="352" height="161" /> 
      <br clear="all" />
    </div>
    
    <ul>
      <li class="western"><strong>Hessian Threshold</strong>: Threshold for the keypoint detector. Only features, whose hessian is larger than hessianThreshold are retained by the detector.</li>
      <li class="western"><strong>Octaves</strong>: The number of a gaussian pyramid octaves that the detector uses.&nbsp;It is set to 4 by default. If you want to get very large features, use the larger value. If you want just small features, decrease it.</li>
      <li class="western"><strong>Octave Layers</strong>: The number of images within each octave of a gaussian pyramid.&nbsp;</li>
      <li class="western"><strong>Extended Descriptor</strong>:&nbsp;If checked use extended 128-element descriptors otherwise use basic 64-element descriptors.&nbsp;</li>
      <li class="western"><strong>Upright: </strong>If true&nbsp;the orientation is not computed.</li>
    </ul>

    <h3 class="western"><a name="VGG">VGG (Descriptor)</a></h3>     

    <h4 class="western">Parameters</h4>
    
    <div class="image"><img src="img/vgg.png" width="352" height="159" /> 
      <br clear="all" />
    </div>
    
</body>
</html>